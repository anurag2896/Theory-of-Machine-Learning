{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d167a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ff4aee",
   "metadata": {},
   "source": [
    "## Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8531c66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the data\n",
    "def create_tuples():\n",
    "    data = []\n",
    "\n",
    "    for i in range(-50, 0):\n",
    "        j = (i, -1)\n",
    "        data.append(j)\n",
    "\n",
    "    for i in range(1, 51):\n",
    "        j = (i, 1)\n",
    "        data.append(j)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Function to create corrupt data\n",
    "def create_corrupt_tuples():\n",
    "    data = []\n",
    "\n",
    "    for i in range(-50, -45):\n",
    "        j = (i, 1)\n",
    "        data.append(j)\n",
    "\n",
    "    for i in range(-45, 0):\n",
    "        j = (i, -1)\n",
    "        data.append(j)\n",
    "\n",
    "    for i in range(1, 46):\n",
    "        j = (i, 1)\n",
    "        data.append(j)\n",
    "\n",
    "    for i in range(46, 51):\n",
    "        j = (i, -1)\n",
    "        data.append(j)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Loss function \n",
    "def loss_fucntion(v, data):\n",
    "\n",
    "    loss = 0\n",
    "    for j in data:\n",
    "        loss += math.log(1 + math.exp(-1 * j[0] * j[1] * v))\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Function to calculate the gradient\n",
    "def gradient(v, data):\n",
    "\n",
    "    gradient = 0\n",
    "    for j in data:\n",
    "        gradient += (-1 * j[0] * j[1]) * math.exp(-1 * j[0] * j[1] * v) / (1 + math.exp(-1 * j[0] * j[1] * v))\n",
    "\n",
    "    return gradient\n",
    "\n",
    "# Gradient Descent function uses the gradient function to reduce the loss \n",
    "def gradient_descent(start_point, lr, n, data):\n",
    "\n",
    "    vector = start_point\n",
    "    print(\"Initial loss is :\", loss_fucntion(vector, data))\n",
    "\n",
    "    for i in range(n):\n",
    "        diff = -lr * gradient(vector, data)\n",
    "        vector += diff\n",
    "        print(\"Loss for iteration {} is : {}\".format(i, loss_fucntion(vector, data)))\n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6031ad98",
   "metadata": {},
   "source": [
    "### Q1-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cf7303d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss is : 2551.03512021427\n",
      "Loss for iteration 0 is : 1901.724659854014\n",
      "Loss for iteration 1 is : 1253.3340798105403\n",
      "Loss for iteration 2 is : 608.3979926654172\n",
      "Loss for iteration 3 is : 51.67631439998525\n",
      "Loss for iteration 4 is : 14.161717351746809\n",
      "Loss for iteration 5 is : 12.616098037073115\n",
      "Loss for iteration 6 is : 11.57629705499606\n",
      "Loss for iteration 7 is : 10.807218608693688\n",
      "Loss for iteration 8 is : 10.204723908538499\n",
      "Loss for iteration 9 is : 9.71404869869955\n",
      "Loss for iteration 10 is : 9.303071092254005\n",
      "Loss for iteration 11 is : 8.95145329911838\n",
      "Loss for iteration 12 is : 8.645565832444529\n",
      "Loss for iteration 13 is : 8.375863851924626\n",
      "Loss for iteration 14 is : 8.135423839982826\n",
      "Loss for iteration 15 is : 7.919077149636536\n",
      "Loss for iteration 16 is : 7.722871537399784\n",
      "Loss for iteration 17 is : 7.543722865512498\n",
      "Loss for iteration 18 is : 7.379182094306985\n",
      "Loss for iteration 19 is : 7.227274853060721\n",
      "Loss for iteration 20 is : 7.086388200597224\n",
      "Loss for iteration 21 is : 6.955188939454654\n",
      "Loss for iteration 22 is : 6.8325635523188195\n",
      "Loss for iteration 23 is : 6.717573279818634\n",
      "Loss for iteration 24 is : 6.609420007844313\n",
      "Loss for iteration 25 is : 6.507420006422211\n",
      "Loss for iteration 26 is : 6.4109834611795335\n",
      "Loss for iteration 27 is : 6.319598339148214\n",
      "Loss for iteration 28 is : 6.232817539735167\n",
      "Loss for iteration 29 is : 6.150248565101202\n",
      "Loss for iteration 30 is : 6.071545143656071\n",
      "Loss for iteration 31 is : 5.9964003827983445\n",
      "Loss for iteration 32 is : 5.924541130077767\n",
      "Loss for iteration 33 is : 5.855723297438264\n",
      "Loss for iteration 34 is : 5.789727959118132\n",
      "Loss for iteration 35 is : 5.726358075649534\n",
      "Loss for iteration 36 is : 5.665435728053034\n",
      "Loss for iteration 37 is : 5.606799770476124\n",
      "Loss for iteration 38 is : 5.550303828113685\n",
      "Loss for iteration 39 is : 5.495814581670928\n",
      "Loss for iteration 40 is : 5.443210290903779\n",
      "Loss for iteration 41 is : 5.392379518649102\n",
      "Loss for iteration 42 is : 5.343220023793366\n",
      "Loss for iteration 43 is : 5.295637797241901\n",
      "Loss for iteration 44 is : 5.249546219455272\n",
      "Loss for iteration 45 is : 5.204865321755331\n",
      "Loss for iteration 46 is : 5.161521136554119\n",
      "Loss for iteration 47 is : 5.11944512406528\n",
      "Loss for iteration 48 is : 5.078573665030573\n",
      "Loss for iteration 49 is : 5.038847610618682\n",
      "Loss for iteration 50 is : 5.0002118819973305\n",
      "Loss for iteration 51 is : 4.962615113196295\n",
      "Loss for iteration 52 is : 4.926009331809907\n",
      "Loss for iteration 53 is : 4.890349672867687\n",
      "Loss for iteration 54 is : 4.8555941218568215\n",
      "Loss for iteration 55 is : 4.821703283433283\n",
      "Loss for iteration 56 is : 4.788640172826094\n",
      "Loss for iteration 57 is : 4.756370027336773\n",
      "Loss for iteration 58 is : 4.724860135674269\n",
      "Loss for iteration 59 is : 4.694079683155047\n",
      "Loss for iteration 60 is : 4.663999611045798\n",
      "Loss for iteration 61 is : 4.63459248853932\n",
      "Loss for iteration 62 is : 4.605832396037555\n",
      "Loss for iteration 63 is : 4.577694818574477\n",
      "Loss for iteration 64 is : 4.55015654834886\n",
      "Loss for iteration 65 is : 4.523195595456252\n",
      "Loss for iteration 66 is : 4.496791106013268\n",
      "Loss for iteration 67 is : 4.470923286958001\n",
      "Loss for iteration 68 is : 4.445573336889349\n",
      "Loss for iteration 69 is : 4.420723382377694\n",
      "Loss for iteration 70 is : 4.396356419240032\n",
      "Loss for iteration 71 is : 4.372456258326447\n",
      "Loss for iteration 72 is : 4.349007475411953\n",
      "Loss for iteration 73 is : 4.325995364829501\n",
      "Loss for iteration 74 is : 4.30340589651681\n",
      "Loss for iteration 75 is : 4.281225676182512\n",
      "Loss for iteration 76 is : 4.2594419083259965\n",
      "Loss for iteration 77 is : 4.238042361871342\n",
      "Loss for iteration 78 is : 4.217015338198556\n",
      "Loss for iteration 79 is : 4.1963496413760994\n",
      "Loss for iteration 80 is : 4.176034550416817\n",
      "Loss for iteration 81 is : 4.156059793395954\n",
      "Loss for iteration 82 is : 4.13641552328462\n",
      "Loss for iteration 83 is : 4.117092295365224\n",
      "Loss for iteration 84 is : 4.098081046107403\n",
      "Loss for iteration 85 is : 4.079373073393497\n",
      "Loss for iteration 86 is : 4.060960017992447\n",
      "Loss for iteration 87 is : 4.042833846189465\n",
      "Loss for iteration 88 is : 4.024986833486862\n",
      "Loss for iteration 89 is : 4.007411549298416\n",
      "Loss for iteration 90 is : 3.990100842566219\n",
      "Loss for iteration 91 is : 3.973047828234566\n",
      "Loss for iteration 92 is : 3.9562458745209885\n",
      "Loss for iteration 93 is : 3.939688590929165\n",
      "Loss for iteration 94 is : 3.9233698169529276\n",
      "Loss for iteration 95 is : 3.907283611424536\n",
      "Loss for iteration 96 is : 3.891424242463968\n",
      "Loss for iteration 97 is : 3.8757861779894647\n",
      "Loss for iteration 98 is : 3.860364076752377\n",
      "Loss for iteration 99 is : 3.8451527798623037\n",
      "\n",
      "Final value for the weights is: 0.3649009809906515\n"
     ]
    }
   ],
   "source": [
    "data_clean = create_tuples()\n",
    "w = gradient_descent(start_point=-1, lr=0.0001, n=100, data=data_clean)\n",
    "print(\"\\nFinal value for the weights is:\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49010617",
   "metadata": {},
   "source": [
    "### Q1-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54a9c716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss is : 2071.0351202142706\n",
      "Loss for iteration 0 is : 1643.2801100038082\n",
      "Loss for iteration 1 is : 1216.0280714839773\n",
      "Loss for iteration 2 is : 790.0188510127538\n",
      "Loss for iteration 3 is : 368.97513917907736\n",
      "Loss for iteration 4 is : 55.189770486758995\n",
      "Loss for iteration 5 is : 53.23083146737826\n",
      "Loss for iteration 6 is : 52.99046906395331\n",
      "Loss for iteration 7 is : 52.990416430814534\n",
      "Loss for iteration 8 is : 52.9904158801311\n",
      "Loss for iteration 9 is : 52.99041587452637\n",
      "Loss for iteration 10 is : 52.99041587446917\n",
      "Loss for iteration 11 is : 52.990415874468596\n",
      "Loss for iteration 12 is : 52.99041587446855\n",
      "Loss for iteration 13 is : 52.99041587446856\n",
      "Loss for iteration 14 is : 52.990415874468574\n",
      "Loss for iteration 15 is : 52.990415874468574\n",
      "Loss for iteration 16 is : 52.99041587446859\n",
      "Loss for iteration 17 is : 52.990415874468596\n",
      "Loss for iteration 18 is : 52.99041587446859\n",
      "Loss for iteration 19 is : 52.99041587446858\n",
      "Loss for iteration 20 is : 52.99041587446858\n",
      "Loss for iteration 21 is : 52.99041587446858\n",
      "Loss for iteration 22 is : 52.99041587446858\n",
      "Loss for iteration 23 is : 52.99041587446858\n",
      "Loss for iteration 24 is : 52.99041587446858\n",
      "Loss for iteration 25 is : 52.99041587446858\n",
      "Loss for iteration 26 is : 52.99041587446858\n",
      "Loss for iteration 27 is : 52.99041587446858\n",
      "Loss for iteration 28 is : 52.99041587446858\n",
      "Loss for iteration 29 is : 52.99041587446858\n",
      "Loss for iteration 30 is : 52.99041587446858\n",
      "Loss for iteration 31 is : 52.99041587446858\n",
      "Loss for iteration 32 is : 52.99041587446858\n",
      "Loss for iteration 33 is : 52.99041587446858\n",
      "Loss for iteration 34 is : 52.99041587446858\n",
      "Loss for iteration 35 is : 52.99041587446858\n",
      "Loss for iteration 36 is : 52.99041587446858\n",
      "Loss for iteration 37 is : 52.99041587446858\n",
      "Loss for iteration 38 is : 52.99041587446858\n",
      "Loss for iteration 39 is : 52.99041587446858\n",
      "Loss for iteration 40 is : 52.99041587446858\n",
      "Loss for iteration 41 is : 52.99041587446858\n",
      "Loss for iteration 42 is : 52.99041587446858\n",
      "Loss for iteration 43 is : 52.99041587446858\n",
      "Loss for iteration 44 is : 52.99041587446858\n",
      "Loss for iteration 45 is : 52.99041587446858\n",
      "Loss for iteration 46 is : 52.99041587446858\n",
      "Loss for iteration 47 is : 52.99041587446858\n",
      "Loss for iteration 48 is : 52.99041587446858\n",
      "Loss for iteration 49 is : 52.99041587446858\n",
      "Loss for iteration 50 is : 52.99041587446858\n",
      "Loss for iteration 51 is : 52.99041587446858\n",
      "Loss for iteration 52 is : 52.99041587446858\n",
      "Loss for iteration 53 is : 52.99041587446858\n",
      "Loss for iteration 54 is : 52.99041587446858\n",
      "Loss for iteration 55 is : 52.99041587446858\n",
      "Loss for iteration 56 is : 52.99041587446858\n",
      "Loss for iteration 57 is : 52.99041587446858\n",
      "Loss for iteration 58 is : 52.99041587446858\n",
      "Loss for iteration 59 is : 52.99041587446858\n",
      "Loss for iteration 60 is : 52.99041587446858\n",
      "Loss for iteration 61 is : 52.99041587446858\n",
      "Loss for iteration 62 is : 52.99041587446858\n",
      "Loss for iteration 63 is : 52.99041587446858\n",
      "Loss for iteration 64 is : 52.99041587446858\n",
      "Loss for iteration 65 is : 52.99041587446858\n",
      "Loss for iteration 66 is : 52.99041587446858\n",
      "Loss for iteration 67 is : 52.99041587446858\n",
      "Loss for iteration 68 is : 52.99041587446858\n",
      "Loss for iteration 69 is : 52.99041587446858\n",
      "Loss for iteration 70 is : 52.99041587446858\n",
      "Loss for iteration 71 is : 52.99041587446858\n",
      "Loss for iteration 72 is : 52.99041587446858\n",
      "Loss for iteration 73 is : 52.99041587446858\n",
      "Loss for iteration 74 is : 52.99041587446858\n",
      "Loss for iteration 75 is : 52.99041587446858\n",
      "Loss for iteration 76 is : 52.99041587446858\n",
      "Loss for iteration 77 is : 52.99041587446858\n",
      "Loss for iteration 78 is : 52.99041587446858\n",
      "Loss for iteration 79 is : 52.99041587446858\n",
      "Loss for iteration 80 is : 52.99041587446858\n",
      "Loss for iteration 81 is : 52.99041587446858\n",
      "Loss for iteration 82 is : 52.99041587446858\n",
      "Loss for iteration 83 is : 52.99041587446858\n",
      "Loss for iteration 84 is : 52.99041587446858\n",
      "Loss for iteration 85 is : 52.99041587446858\n",
      "Loss for iteration 86 is : 52.99041587446858\n",
      "Loss for iteration 87 is : 52.99041587446858\n",
      "Loss for iteration 88 is : 52.99041587446858\n",
      "Loss for iteration 89 is : 52.99041587446858\n",
      "Loss for iteration 90 is : 52.99041587446858\n",
      "Loss for iteration 91 is : 52.99041587446858\n",
      "Loss for iteration 92 is : 52.99041587446858\n",
      "Loss for iteration 93 is : 52.99041587446858\n",
      "Loss for iteration 94 is : 52.99041587446858\n",
      "Loss for iteration 95 is : 52.99041587446858\n",
      "Loss for iteration 96 is : 52.99041587446858\n",
      "Loss for iteration 97 is : 52.99041587446858\n",
      "Loss for iteration 98 is : 52.99041587446858\n",
      "Loss for iteration 99 is : 52.99041587446858\n",
      "\n",
      "Final values for the weights (corrupt) is: 0.046148698970806534\n"
     ]
    }
   ],
   "source": [
    "data_corrupt = create_corrupt_tuples()\n",
    "w = gradient_descent(start_point=-1, lr=0.0001, n=100, data=data_corrupt)\n",
    "print(\"\\nFinal values for the weights (corrupt) is:\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaa69d9",
   "metadata": {},
   "source": [
    "## Q3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4214216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a list of eta values\n",
    "def fetch_eta():\n",
    "\n",
    "    mean = 0, \n",
    "    std_dev = math.pow(0.5, 0.5)\n",
    "    eta = np.random.normal(mean, std_dev, 1000)\n",
    "\n",
    "    return eta\n",
    "\n",
    "# Function to create a matrix \n",
    "def create_matrix():\n",
    "\n",
    "    matrix = [[0] * 500] * 1000\n",
    "    for row in range(1000):\n",
    "        for col in range(500):\n",
    "            matrix[row][col] = np.random.uniform(-1, 1)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "# Function to create an input array\n",
    "def create_input():\n",
    "    N = 500\n",
    "    x = [0] * N\n",
    "    for i in range(N):\n",
    "        x[i] = np.random.uniform(-1, 1)\n",
    "\n",
    "    return x\n",
    "\n",
    "# Function to calculate the gradient value\n",
    "def gradient(A, weight, B):\n",
    "    first_term = np.matmul(np.transpose(A), A)\n",
    "    first_term = np.matmul(first_term, weight)\n",
    "    sec_term = np.matmul(np.transpose(A), B)\n",
    "    grad = 2 * (first_term - sec_term)\n",
    "    return grad\n",
    "\n",
    "# Function to calculate the loss value\n",
    "def loss_function(a_mat, x_vec, b_vec):\n",
    "    final_loss = 0\n",
    "    ax = np.matmul(a_mat, x_vec)\n",
    "    l = ax-b_vec\n",
    "    l_trans = np.transpose(l)\n",
    "    final_loss = np.matmul(l_trans, l)\n",
    "    return final_loss\n",
    "\n",
    "# Function which emulate the gradient descent for n=iterations\n",
    "def gradient_descent(A, B, start, learning_rate, iterations):\n",
    "    weight = start\n",
    "    print(\"Initial Loss value\",loss_function(A, weight, B))\n",
    "    for i in range(iterations):\n",
    "        diff = -1 * learning_rate * gradient(A, weight, B)\n",
    "        weight += diff\n",
    "        print(\"value of loss at iteration {} is {}\".format(i, loss_function(A, weight, B)))\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b347100",
   "metadata": {},
   "source": [
    "### Q3.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb83bab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Loss value 5625.941273822504\n",
      "value of loss at iteration 0 is 2806.4986803276574\n",
      "value of loss at iteration 1 is 1548.4572139001373\n",
      "value of loss at iteration 2 is 987.1163535106704\n",
      "value of loss at iteration 3 is 736.6448309482266\n",
      "value of loss at iteration 4 is 624.8838884591651\n",
      "value of loss at iteration 5 is 575.0159109012388\n",
      "value of loss at iteration 6 is 552.7647099867405\n",
      "value of loss at iteration 7 is 542.8361753562308\n",
      "value of loss at iteration 8 is 538.4060414372561\n",
      "value of loss at iteration 9 is 536.429305970197\n",
      "value of loss at iteration 10 is 535.547282271096\n",
      "value of loss at iteration 11 is 535.1537213628507\n",
      "value of loss at iteration 12 is 534.9781136227676\n",
      "value of loss at iteration 13 is 534.8997570641486\n",
      "value of loss at iteration 14 is 534.8647941959078\n",
      "value of loss at iteration 15 is 534.8491936874478\n",
      "value of loss at iteration 16 is 534.8422327063711\n",
      "value of loss at iteration 17 is 534.839126701354\n",
      "value of loss at iteration 18 is 534.8377407951057\n",
      "value of loss at iteration 19 is 534.8371224006995\n",
      "value of loss at iteration 20 is 534.8368464717595\n",
      "value of loss at iteration 21 is 534.8367233516614\n",
      "value of loss at iteration 22 is 534.8366684152038\n",
      "value of loss at iteration 23 is 534.8366439024362\n",
      "value of loss at iteration 24 is 534.8366329647854\n",
      "value of loss at iteration 25 is 534.8366280843817\n",
      "value of loss at iteration 26 is 534.8366259067348\n",
      "value of loss at iteration 27 is 534.8366249350642\n",
      "value of loss at iteration 28 is 534.8366245015023\n",
      "value of loss at iteration 29 is 534.8366243080462\n",
      "value of loss at iteration 30 is 534.8366242217257\n",
      "value of loss at iteration 31 is 534.8366241832091\n",
      "value of loss at iteration 32 is 534.836624166023\n",
      "value of loss at iteration 33 is 534.8366241583547\n",
      "value of loss at iteration 34 is 534.8366241549329\n",
      "value of loss at iteration 35 is 534.8366241534064\n",
      "value of loss at iteration 36 is 534.8366241527249\n",
      "value of loss at iteration 37 is 534.8366241524208\n",
      "value of loss at iteration 38 is 534.8366241522854\n",
      "value of loss at iteration 39 is 534.8366241522249\n",
      "value of loss at iteration 40 is 534.8366241521978\n",
      "value of loss at iteration 41 is 534.8366241521857\n",
      "value of loss at iteration 42 is 534.8366241521805\n",
      "value of loss at iteration 43 is 534.836624152178\n",
      "value of loss at iteration 44 is 534.836624152177\n",
      "value of loss at iteration 45 is 534.8366241521765\n",
      "value of loss at iteration 46 is 534.8366241521761\n",
      "value of loss at iteration 47 is 534.8366241521761\n",
      "value of loss at iteration 48 is 534.836624152176\n",
      "value of loss at iteration 49 is 534.8366241521762\n"
     ]
    }
   ],
   "source": [
    "A = create_matrix()\n",
    "arr = np.array(A)\n",
    "\n",
    "X = create_input()\n",
    "input_arr = np.array(X)\n",
    "\n",
    "eta = fetch_eta()\n",
    "eta_list = np.array(eta)\n",
    "\n",
    "B = np.matmul(arr, input_arr)\n",
    "B = np.add(B, eta_list)\n",
    "\n",
    "w = [0]*500\n",
    "\n",
    "final_ans = gradient_descent(A, B, w, 0.000001, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7dba4bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Loss value 5625.941273822504\n",
      "value of loss at iteration 0 is 5611832950009.042\n",
      "value of loss at iteration 1 is 6.185822374490691e+21\n",
      "value of loss at iteration 2 is 6.81852057843861e+30\n",
      "value of loss at iteration 3 is 7.515932411883849e+39\n",
      "value of loss at iteration 4 is 8.284676913440005e+48\n",
      "value of loss at iteration 5 is 9.132050130142449e+57\n",
      "value of loss at iteration 6 is 1.0066094363214868e+67\n",
      "value of loss at iteration 7 is 1.1095674496430497e+76\n",
      "value of loss at iteration 8 is 1.223056213148974e+85\n",
      "value of loss at iteration 9 is 1.3481528328931534e+94\n",
      "value of loss at iteration 10 is 1.4860445834769276e+103\n",
      "value of loss at iteration 11 is 1.638040176307013e+112\n",
      "value of loss at iteration 12 is 1.8055821803933112e+121\n",
      "value of loss at iteration 13 is 1.9902607135704375e+130\n",
      "value of loss at iteration 14 is 2.1938285340848148e+139\n",
      "value of loss at iteration 15 is 2.4182176757790705e+148\n",
      "value of loss at iteration 16 is 2.6655577847563375e+157\n",
      "value of loss at iteration 17 is 2.9381963315548303e+166\n",
      "value of loss at iteration 18 is 3.2387208906639425e+175\n",
      "value of loss at iteration 19 is 3.569983698833467e+184\n",
      "value of loss at iteration 20 is 3.9351287252554675e+193\n",
      "value of loss at iteration 21 is 4.337621510538198e+202\n",
      "value of loss at iteration 22 is 4.781282057669457e+211\n",
      "value of loss at iteration 23 is 5.270321087133174e+220\n",
      "value of loss at iteration 24 is 5.809380000271223e+229\n",
      "value of loss at iteration 25 is 6.403574930177406e+238\n",
      "value of loss at iteration 26 is 7.05854529820427e+247\n",
      "value of loss at iteration 27 is 7.780507336926182e+256\n",
      "value of loss at iteration 28 is 8.57631308753702e+265\n",
      "value of loss at iteration 29 is 9.45351543161929e+274\n",
      "value of loss at iteration 30 is 1.0420439774491598e+284\n",
      "value of loss at iteration 31 is 1.1486263060471614e+293\n",
      "value of loss at iteration 32 is 1.2661100869976593e+302\n",
      "value of loss at iteration 33 is inf\n",
      "value of loss at iteration 34 is inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/u1317679/miniconda3/envs/tml/lib/python3.7/site-packages/ipykernel_launcher.py:43: RuntimeWarning: overflow encountered in matmul\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of loss at iteration 35 is inf\n",
      "value of loss at iteration 36 is inf\n",
      "value of loss at iteration 37 is inf\n",
      "value of loss at iteration 38 is inf\n",
      "value of loss at iteration 39 is inf\n",
      "value of loss at iteration 40 is inf\n",
      "value of loss at iteration 41 is inf\n",
      "value of loss at iteration 42 is inf\n",
      "value of loss at iteration 43 is inf\n",
      "value of loss at iteration 44 is inf\n",
      "value of loss at iteration 45 is inf\n",
      "value of loss at iteration 46 is inf\n",
      "value of loss at iteration 47 is inf\n",
      "value of loss at iteration 48 is inf\n",
      "value of loss at iteration 49 is inf\n"
     ]
    }
   ],
   "source": [
    "final_ans = gradient_descent(A, B, w, 0.1, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b845327",
   "metadata": {},
   "source": [
    "### Q3.D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e53907f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of the distance is  156.66645046466\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate the distance at 50 iterations\n",
    "def get_distance(input_arr, final_ans):\n",
    "    d = final_ans - input_arr\n",
    "    dT = np.transpose(d)\n",
    "    dist = np.matmul(dT, d)\n",
    "    return dist\n",
    "\n",
    "print(\"value of the distance is \", get_distance(input_arr, final_ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df915f1",
   "metadata": {},
   "source": [
    "### Q3.E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a9e7bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at start 148142.5051272421\n",
      "loss at 0:69033.16205779527\n",
      "loss at 1:32309.38139049038\n",
      "loss at 2:15261.634383965406\n",
      "loss at 3:7347.806539513007\n",
      "loss at 4:3674.085247915959\n",
      "loss at 5:1968.6870015042416\n",
      "loss at 6:1177.014757472945\n",
      "loss at 7:809.5082566962185\n",
      "loss at 8:638.9060546720415\n",
      "loss at 9:559.7098737229117\n",
      "loss at 10:522.9457815686993\n",
      "loss at 11:505.8793213464406\n",
      "loss at 12:497.95680653780096\n",
      "loss at 13:494.27905262306103\n",
      "loss at 14:492.57178237061373\n",
      "loss at 15:491.77924111242567\n",
      "loss at 16:491.4113312018357\n",
      "loss at 17:491.2405417307365\n",
      "loss at 18:491.16125861658605\n",
      "loss at 19:491.12445416869525\n",
      "loss at 20:491.107368974716\n",
      "loss at 21:491.0994377634076\n",
      "loss at 22:491.095755972443\n",
      "loss at 23:491.0940468281296\n",
      "loss at 24:491.0932534169033\n",
      "loss at 25:491.09288510314025\n",
      "loss at 26:491.09271412619427\n",
      "loss at 27:491.09263475605155\n",
      "loss at 28:491.09259791120377\n",
      "loss at 29:491.0925808072554\n",
      "loss at 30:491.0925728673381\n",
      "loss at 31:491.09256918150544\n",
      "loss at 32:491.0925674704853\n",
      "loss at 33:491.09256667620315\n",
      "loss at 34:491.09256630748496\n",
      "loss at 35:491.09256613632033\n",
      "loss at 36:491.09256605686295\n",
      "loss at 37:491.09256601997777\n",
      "loss at 38:491.09256600285494\n",
      "loss at 39:491.09256599490647\n",
      "loss at 40:491.09256599121665\n",
      "loss at 41:491.0925659895036\n",
      "loss at 42:491.09256598870843\n",
      "loss at 43:491.09256598833946\n",
      "loss at 44:491.09256598816813\n",
      "loss at 45:491.09256598808855\n",
      "loss at 46:491.09256598805155\n",
      "loss at 47:491.09256598803444\n",
      "loss at 48:491.0925659880264\n",
      "loss at 49:491.0925659880227\n",
      "Time by the Gradient Descent : 5.722322225570679\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "final_x = gradient_descent(A, B, w, 0.000001, 50)\n",
    "end = time.time()\n",
    "print(\"Time for the Gradient Descent :\", (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d31d54e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time using the Regression Formula : 0.20625591278076172\n"
     ]
    }
   ],
   "source": [
    "start1 = time.time()\n",
    "\n",
    "X_star = np.matmul(np.transpose(A), A)\n",
    "X_star = np.linalg.inv(X_star)\n",
    "X_star = np.matmul(X_star, np.transpose(A))\n",
    "X_star = np.matmul(X_star, A)\n",
    "\n",
    "end1 = time.time()\n",
    "print(\"Time using the Regression Formula :\", (end1 - start1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2efdff58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.05865237e+224  4.28856948e+222  3.90668099e+223 -2.17598092e+223\n",
      "  9.34692169e+223  3.57782211e+222  7.12228256e+223  3.04468041e+222\n",
      " -2.83958092e+223  8.11958099e+223 -4.74284825e+223  3.02266269e+223\n",
      "  8.48765818e+223 -7.78888013e+223  6.28337182e+223 -6.55842323e+223\n",
      "  7.58690412e+223 -5.08443412e+223 -4.46333198e+223 -1.07779682e+224\n",
      " -7.17297632e+223 -8.09197086e+223 -9.85712365e+223 -9.76813059e+223\n",
      " -6.45260108e+223  5.78892379e+223  1.31050792e+223  7.13518384e+223\n",
      " -9.08314449e+222  2.28047053e+223 -3.52296912e+223  1.60995076e+222\n",
      "  9.51883722e+223 -3.62158725e+223  1.05443533e+224 -5.55792014e+223\n",
      " -7.05720748e+223  5.46107126e+222  8.92718464e+223 -5.52847997e+223\n",
      "  1.05006290e+224  1.10008586e+223  8.13536149e+222 -6.05778117e+223\n",
      "  3.18450212e+222  6.89792003e+223  5.84180993e+223  1.03372044e+224\n",
      "  5.98595673e+223 -4.55596706e+223  9.09838034e+223 -1.69116531e+223\n",
      "  9.07220077e+223 -5.57796359e+223 -7.20960714e+222  1.20124936e+223\n",
      "  7.39331935e+223 -9.10213552e+223  8.49482608e+222  4.08555579e+223\n",
      " -5.24958237e+222  1.09336436e+224 -4.62083509e+223 -6.84487107e+223\n",
      " -3.30088987e+223 -6.45606864e+222 -4.02932252e+223 -4.75932948e+223\n",
      " -7.53871608e+223 -3.12580785e+223 -3.56515479e+223  9.69632781e+223\n",
      " -1.55355047e+223 -7.06726090e+223 -5.18496621e+223 -7.76983021e+223\n",
      "  7.14419854e+223  8.43213477e+222  6.53410554e+223 -3.27759279e+223\n",
      " -8.34954817e+223  1.03575322e+224  3.60108459e+223  1.01342274e+224\n",
      " -9.69855066e+223  1.26417553e+223  5.59101394e+223  4.60365362e+223\n",
      "  6.73199765e+223  4.49720253e+223 -5.99910924e+223  8.48709122e+223\n",
      " -5.13699579e+223 -1.02764635e+224 -3.65251548e+223 -6.76502750e+223\n",
      "  3.49214158e+223  5.79612310e+223  2.09754688e+223 -4.55425421e+223\n",
      " -5.74938244e+221 -4.79486500e+223 -5.80386135e+223 -7.26820754e+223\n",
      " -7.74064596e+223 -9.65257012e+223 -5.11606065e+223  2.20787506e+223\n",
      " -1.75365084e+223  1.48629344e+223 -5.21653497e+223  9.99093304e+221\n",
      "  3.40915187e+223 -7.58063348e+223 -9.54043971e+223 -7.19846657e+223\n",
      " -2.98335691e+223  2.79280018e+223  2.05496934e+223  8.08905516e+223\n",
      " -3.59721885e+223  4.60979811e+223 -3.75848540e+223 -4.02553205e+223\n",
      "  1.00819336e+224  1.00991892e+224 -8.36498039e+223 -6.47653321e+223\n",
      " -5.08905976e+223 -6.73789011e+223  1.08796036e+224  1.23111378e+223\n",
      "  7.34139036e+223 -9.57495236e+222 -1.06361196e+224  6.83176740e+223\n",
      "  7.94983703e+223 -8.63510162e+223 -4.91400327e+222  6.47699862e+223\n",
      "  6.30729609e+223 -3.45029365e+223 -1.33148163e+222 -4.00898155e+222\n",
      " -5.29327221e+223 -3.33201324e+223  6.07146283e+223  9.73501215e+223\n",
      "  1.04738535e+224 -3.90130288e+223 -7.50059008e+223  6.73859339e+223\n",
      "  2.62179401e+223  1.01048156e+224 -1.49711552e+223  8.37910827e+221\n",
      " -3.52374154e+223  3.80181259e+223 -1.82457706e+223 -2.68478997e+223\n",
      "  6.87223347e+223  1.06207883e+223 -3.36067879e+223  5.33016332e+221\n",
      "  3.57631339e+223 -2.36276719e+223  1.07694084e+224 -2.57311761e+222\n",
      "  2.46127325e+223  1.03054665e+224 -1.06029923e+224  9.30453117e+223\n",
      " -9.73972774e+223 -1.06855356e+224  8.57149389e+223 -9.37599257e+223\n",
      "  7.98754658e+223  1.62959839e+223 -1.70575418e+223 -2.80999233e+223\n",
      "  4.88613311e+223 -5.33443716e+223  8.23847418e+223 -5.35229929e+223\n",
      "  8.72098638e+223  5.10006704e+223  6.53017043e+223  9.70329871e+223\n",
      " -2.93992510e+223 -7.28823309e+223  4.79874327e+223 -1.10708161e+223\n",
      "  4.10167727e+223 -1.12659292e+223 -7.82845494e+223 -3.43901642e+223\n",
      "  6.79915053e+223  7.14140435e+223 -3.92107282e+223 -1.08900705e+224\n",
      "  4.86499913e+223 -7.08320132e+223  5.78871872e+223 -6.53457498e+223\n",
      " -5.60973082e+223  9.20705911e+223 -1.10541727e+224 -3.08841386e+222\n",
      " -1.01507807e+224  4.18690353e+223  1.03232607e+224 -4.61603933e+223\n",
      "  1.50438488e+222  7.98722515e+223  1.89445052e+223 -4.08120586e+223\n",
      "  5.39461110e+223 -7.62484227e+223  4.78199395e+223  1.43066209e+223\n",
      " -9.19110043e+223 -3.35612297e+223 -9.10280528e+223  2.84910072e+223\n",
      " -9.20277821e+223 -5.00887583e+223  7.02019185e+223  2.69887523e+223\n",
      "  2.20293051e+223  8.71419622e+223  1.44893014e+223  7.85913871e+223\n",
      "  8.31767105e+222 -9.24587989e+223  9.18996198e+223 -8.69556890e+223\n",
      "  4.43038646e+222 -1.24473652e+223 -6.09119957e+222  1.07334133e+224\n",
      " -2.01782332e+223  4.04919467e+223  7.73799696e+223 -1.33382523e+222\n",
      " -1.01944120e+224 -9.87067683e+223 -3.36542426e+223 -2.63695567e+223\n",
      "  1.21131021e+223 -8.07198712e+223 -7.88521536e+222 -6.71654893e+223\n",
      "  1.86786736e+223  8.26644408e+223 -5.97214005e+221  9.94281257e+223\n",
      "  1.83724673e+223 -9.24345967e+223  2.68158756e+223  7.30904723e+221\n",
      "  6.37481040e+223 -8.25173994e+223 -1.15741549e+223 -2.65572352e+223\n",
      " -7.14522883e+223  9.09846586e+223  8.40861409e+223 -4.99300400e+223\n",
      "  1.32873114e+223 -7.26315287e+222  2.27566906e+223  1.57169536e+222\n",
      "  8.88416021e+223 -7.19123627e+223 -4.63491313e+223  7.26375880e+223\n",
      "  7.21865974e+223 -5.11832840e+223  1.08219656e+224 -2.14722536e+223\n",
      "  8.40875223e+223 -8.88698117e+223 -5.01461762e+223  4.71909200e+223\n",
      "  1.07336449e+223  7.48866841e+223 -4.42804498e+223 -8.72649934e+223\n",
      "  1.44183598e+223  1.01914687e+224  7.42295203e+223 -1.02944562e+224\n",
      "  1.55213776e+223 -3.38951688e+223  3.78576625e+223  7.31407070e+223\n",
      " -1.04831256e+224 -1.09293265e+224 -7.66687250e+223 -3.91128124e+223\n",
      "  9.49710130e+223  2.08635391e+223 -7.75431948e+223  1.07608666e+224\n",
      " -3.56258113e+223 -6.77428484e+223 -1.11483816e+224  2.40132080e+223\n",
      "  4.32084241e+223  2.72281100e+223 -8.13370584e+223  1.00318030e+223\n",
      "  6.10943392e+223 -3.60490863e+223 -4.14707474e+222 -2.53190091e+223\n",
      "  1.66747178e+223 -3.59799585e+223  7.38675859e+223  2.50651041e+223\n",
      " -8.98590161e+223 -7.27004785e+223 -7.22798288e+223 -9.36111652e+223\n",
      "  1.10044668e+224 -2.92240825e+222  4.72731533e+223 -1.10892105e+224\n",
      "  8.36098204e+223 -6.84667877e+223 -3.79905760e+223 -2.50243429e+223\n",
      " -6.58242702e+223  3.74100309e+223 -4.71169168e+223 -1.15726567e+223\n",
      "  4.74563127e+223 -5.07029215e+223  8.30447739e+223  1.71115747e+223\n",
      " -7.96067357e+223  4.33802397e+223  7.05486021e+223 -1.02174659e+224\n",
      " -7.70079382e+223  7.58562751e+223  8.09231329e+222 -9.47293407e+223\n",
      "  3.50997051e+223  4.08109135e+223  2.78904072e+223 -9.15221637e+223\n",
      " -8.86116702e+223  6.38859878e+222 -5.20780092e+221  4.16885883e+223\n",
      "  3.71260303e+223 -1.11469717e+224 -1.09136352e+224  4.66116490e+223\n",
      " -3.37310582e+223 -2.88314495e+223 -5.89137550e+223 -8.53539258e+223\n",
      "  4.67976961e+223  9.80121572e+223 -1.46019239e+223 -6.70021309e+223\n",
      " -5.12670491e+223 -9.77521175e+222 -3.31106118e+223  1.31140825e+223\n",
      " -1.08590161e+224 -1.81724431e+223 -6.54695908e+223 -6.80951554e+222\n",
      "  1.10308994e+224 -7.34709744e+223 -9.65032740e+223  1.04120231e+223\n",
      "  8.75459683e+223  3.33629709e+223  9.89082335e+223  2.11912318e+222\n",
      " -1.10026458e+224 -3.43559843e+223 -2.77746036e+223 -4.58901189e+223\n",
      " -9.73617666e+223 -2.73963130e+223 -4.85484102e+223 -8.42759880e+223\n",
      " -8.45074545e+223 -7.82646837e+223 -9.86616294e+223  7.72404749e+223\n",
      " -2.73052961e+223 -4.12454600e+223  5.17627275e+223 -6.91572716e+222\n",
      " -3.90997286e+223  4.26861802e+223 -9.12774084e+223  1.25253767e+223\n",
      " -8.82746224e+223 -1.05371631e+224  5.21417633e+223 -2.72013849e+222\n",
      " -2.69078228e+223  1.03229041e+224  5.45136056e+223  7.03357901e+223\n",
      " -4.14035828e+223  2.06521396e+223 -1.29612614e+223  7.88734405e+222\n",
      "  5.66000371e+223  1.03369405e+224 -5.76411537e+223  3.35626285e+223\n",
      " -8.10926112e+223 -5.22526248e+223  4.45490450e+223 -1.09031328e+224\n",
      " -1.07456062e+223 -3.97800056e+223  9.20874563e+223 -4.16618380e+223\n",
      " -5.97654012e+223  4.37583454e+222 -1.10898957e+222 -3.37368641e+222\n",
      " -8.00501557e+223 -1.02500163e+224 -1.83623045e+223 -5.39755317e+223\n",
      " -9.29073896e+223 -1.00957358e+224  1.62480814e+223  6.80419593e+222\n",
      "  1.08596439e+224 -4.37405049e+223 -3.18221580e+223  7.24666039e+223\n",
      "  6.13029466e+223 -2.96284332e+223 -4.58597944e+223  8.25195383e+223\n",
      " -1.67048060e+223  2.87582747e+223  4.34572746e+223 -8.17992087e+223\n",
      "  1.08372315e+224 -2.92239613e+223 -3.13383023e+223 -2.70866927e+223\n",
      "  8.54647636e+223  9.89184819e+223 -7.83376823e+223  4.13366128e+223\n",
      "  8.16780630e+223  4.41515000e+223 -5.74146015e+223  2.66614530e+223\n",
      " -9.41822343e+223  4.95008490e+222  9.58684430e+223 -1.03401878e+224\n",
      " -4.46965871e+223  3.36507704e+223 -1.68659938e+223  7.78157853e+221\n",
      " -4.13835464e+223 -9.27597452e+223  4.24094341e+223  6.23265205e+223\n",
      " -2.99090989e+223  9.52436450e+223 -3.53441967e+223  3.15881005e+223\n",
      "  4.51925661e+221 -7.18243500e+223  8.99521542e+222  7.19878000e+223\n",
      " -9.35784598e+223 -4.70595246e+223 -4.18921318e+223  6.25072538e+223\n",
      "  2.49612869e+223  1.00387097e+224 -4.46468828e+223  3.95290696e+222\n",
      "  8.70417835e+223  5.81150683e+223  6.38495148e+222 -7.29340786e+223\n",
      " -5.72305659e+223 -7.56847395e+223 -2.40628494e+223 -2.49082139e+223]\n"
     ]
    }
   ],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f033973",
   "metadata": {},
   "source": [
    "## Q4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a966f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the data\n",
    "def create_data():\n",
    "    arr = []\n",
    "    N = 100\n",
    "\n",
    "    for value in range(N):\n",
    "        tup = (value/N, -1)\n",
    "        arr.append(tup)\n",
    "\n",
    "    for value in range(N):\n",
    "        tup = (value/N, 1)\n",
    "        arr.append(tup)\n",
    "\n",
    "    return arr\n",
    "\n",
    "# Function to calculate the gradient value\n",
    "def gradient(x_val, y_val, a, b, t):\n",
    "#     eta=0.1\n",
    "#     eta=0.1/(1 + t)\n",
    "    eta=0.1/pow((1 + t),0.5)\n",
    "    gradient = (x_val * (1 - (2 * eta)) + 2 * eta * a, y_val * (1 - (2 * eta)) + 2 * eta * b)\n",
    "    return gradient\n",
    "\n",
    "# Function to calculate the loss value\n",
    "def loss_function(x_val, y_val, arr):\n",
    "    loss = 0\n",
    "\n",
    "    for i in range(len(arr)):\n",
    "        loss += pow(x_val - arr[i][0], 2) + pow(y_val - arr[i][1], 2)\n",
    "    loss = loss / 400\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Function which emulate the gradient descent for n=iterations\n",
    "def gradient_descent(arr, iterations):\n",
    "    x_val = 1\n",
    "    y_val = 1\n",
    "\n",
    "    print(\"initial loss:\", loss_function(x_val, y_val, arr))\n",
    "\n",
    "    for i in range(iterations):\n",
    "\n",
    "        random_num = random.randrange(0, 200)\n",
    "\n",
    "        a_i = arr[random_num][0]\n",
    "        b_i = arr[random_num][1]\n",
    "\n",
    "        grad = gradient(x_val, y_val, a_i, b_i, i)\n",
    "\n",
    "        x_val = grad[0]\n",
    "        y_val = grad[1]\n",
    "        print(\"Loss at iteration {} is {}\".format(i, loss_function(grad[0], grad[1], arr)))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "967b9f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss: 1.169175\n",
      "Loss at iteration 0 is 0.8297750000000002\n",
      "Loss at iteration 1 is 0.8623270000000006\n",
      "Loss at iteration 2 is 0.6480778799999999\n",
      "Loss at iteration 3 is 0.5608560432\n",
      "Loss at iteration 4 is 0.5734347076480003\n",
      "Loss at iteration 5 is 0.6142288416947199\n",
      "Loss at iteration 6 is 0.5563405972446204\n",
      "Loss at iteration 7 is 0.5577448998045574\n",
      "Loss at iteration 8 is 0.6046968135325169\n",
      "Loss at iteration 9 is 0.546917629803851\n",
      "Loss at iteration 10 is 0.5528958138295365\n",
      "Loss at iteration 11 is 0.5932412291024487\n",
      "Loss at iteration 12 is 0.5436161584899386\n",
      "Loss at iteration 13 is 0.5539031792319359\n",
      "Loss at iteration 14 is 0.5445345610221203\n",
      "Loss at iteration 15 is 0.5757647753542022\n",
      "Loss at iteration 16 is 0.5436024734075204\n",
      "Loss at iteration 17 is 0.5603713242069093\n",
      "Loss at iteration 18 is 0.6045815990192591\n",
      "Loss at iteration 19 is 0.5451628632026665\n",
      "Loss at iteration 20 is 0.5530121763098191\n",
      "Loss at iteration 21 is 0.5892536228776833\n",
      "Loss at iteration 22 is 0.5426886034791778\n",
      "Loss at iteration 23 is 0.5743179406031648\n",
      "Loss at iteration 24 is 0.6214449791533936\n",
      "Loss at iteration 25 is 0.6724497712925533\n",
      "Loss at iteration 26 is 0.5634883616306952\n",
      "Loss at iteration 27 is 0.6100953842802589\n",
      "Loss at iteration 28 is 0.5472795546052104\n",
      "Loss at iteration 29 is 0.5506985698316693\n",
      "Loss at iteration 30 is 0.5523292900433013\n",
      "Loss at iteration 31 is 0.5524098835574562\n",
      "Loss at iteration 32 is 0.5507967865243745\n",
      "Loss at iteration 33 is 0.5867671102890931\n",
      "Loss at iteration 34 is 0.5435310162843895\n",
      "Loss at iteration 35 is 0.5629505743593493\n",
      "Loss at iteration 36 is 0.5519249875716089\n",
      "Loss at iteration 37 is 0.5813634229488778\n",
      "Loss at iteration 38 is 0.5522572272444984\n",
      "Loss at iteration 39 is 0.5713866695946895\n",
      "Loss at iteration 40 is 0.5483587308666735\n",
      "Loss at iteration 41 is 0.5705178878689936\n",
      "Loss at iteration 42 is 0.5417513847530135\n",
      "Loss at iteration 43 is 0.56226998687916\n",
      "Loss at iteration 44 is 0.6039328977964736\n",
      "Loss at iteration 45 is 0.5481898367877791\n",
      "Loss at iteration 46 is 0.577225217993349\n",
      "Loss at iteration 47 is 0.5420153850828383\n",
      "Loss at iteration 48 is 0.5598086471551507\n",
      "Loss at iteration 49 is 0.5435754785352984\n",
      "Loss at iteration 50 is 0.5702595416411023\n",
      "Loss at iteration 51 is 0.6184305214725709\n",
      "Loss at iteration 52 is 0.6784104012518948\n",
      "Loss at iteration 53 is 0.7314261398881213\n",
      "Loss at iteration 54 is 0.5918654348563755\n",
      "Loss at iteration 55 is 0.5464281555812464\n",
      "Loss at iteration 56 is 0.5670012744348676\n",
      "Loss at iteration 57 is 0.6158513780200391\n",
      "Loss at iteration 58 is 0.5549749886533853\n",
      "Loss at iteration 59 is 0.5824567259392104\n",
      "Loss at iteration 60 is 0.543873700644675\n",
      "Loss at iteration 61 is 0.5701253241439177\n",
      "Loss at iteration 62 is 0.6207083172068797\n",
      "Loss at iteration 63 is 0.6826173555622097\n",
      "Loss at iteration 64 is 0.744538902955572\n",
      "Loss at iteration 65 is 0.7878580761737104\n",
      "Loss at iteration 66 is 0.6076521500032964\n",
      "Loss at iteration 67 is 0.5453621096572121\n",
      "Loss at iteration 68 is 0.5530291798337967\n",
      "Loss at iteration 69 is 0.5499068374394551\n",
      "Loss at iteration 70 is 0.5783673748502655\n",
      "Loss at iteration 71 is 0.5424439849737811\n",
      "Loss at iteration 72 is 0.5656800782629516\n",
      "Loss at iteration 73 is 0.6107676043849984\n",
      "Loss at iteration 74 is 0.6656517418163068\n",
      "Loss at iteration 75 is 0.5660501280125423\n",
      "Loss at iteration 76 is 0.5505576534408704\n",
      "Loss at iteration 77 is 0.5576692325935872\n",
      "Loss at iteration 78 is 0.5971167030916762\n",
      "Loss at iteration 79 is 0.6510738746766424\n",
      "Loss at iteration 80 is 0.5568865644844684\n",
      "Loss at iteration 81 is 0.5437664484231445\n",
      "Loss at iteration 82 is 0.5577805956360167\n",
      "Loss at iteration 83 is 0.5514216743993194\n",
      "Loss at iteration 84 is 0.5777810217202886\n",
      "Loss at iteration 85 is 0.5450323338172054\n",
      "Loss at iteration 86 is 0.5638163916447644\n",
      "Loss at iteration 87 is 0.5429582214647484\n",
      "Loss at iteration 88 is 0.5696228032411235\n",
      "Loss at iteration 89 is 0.5449951421208548\n",
      "Loss at iteration 90 is 0.5779992830233611\n",
      "Loss at iteration 91 is 0.5537642419979437\n",
      "Loss at iteration 92 is 0.5697008871821175\n",
      "Loss at iteration 93 is 0.6202807317036545\n",
      "Loss at iteration 94 is 0.5524212817323142\n",
      "Loss at iteration 95 is 0.5880055018389276\n",
      "Loss at iteration 96 is 0.6353055067913439\n",
      "Loss at iteration 97 is 0.5515163487577824\n",
      "Loss at iteration 98 is 0.590614839125671\n",
      "Loss at iteration 99 is 0.5443430648945576\n",
      "Loss at iteration 100 is 0.5773838716821422\n",
      "Loss at iteration 101 is 0.5421859340523993\n",
      "Loss at iteration 102 is 0.5630286907525178\n",
      "Loss at iteration 103 is 0.6099636412414695\n",
      "Loss at iteration 104 is 0.6629964293824293\n",
      "Loss at iteration 105 is 0.564310975208101\n",
      "Loss at iteration 106 is 0.6161449611915853\n",
      "Loss at iteration 107 is 0.6648542017827082\n",
      "Loss at iteration 108 is 0.5598722067348512\n",
      "Loss at iteration 109 is 0.5429479070445382\n",
      "Loss at iteration 110 is 0.5553809616509696\n",
      "Loss at iteration 111 is 0.5954199994407405\n",
      "Loss at iteration 112 is 0.5453947078746375\n",
      "Loss at iteration 113 is 0.5825148572145462\n",
      "Loss at iteration 114 is 0.6243513019988522\n",
      "Loss at iteration 115 is 0.6826661915370755\n",
      "Loss at iteration 116 is 0.5761666930741169\n",
      "Loss at iteration 117 is 0.614222336147452\n",
      "Loss at iteration 118 is 0.673192569825033\n",
      "Loss at iteration 119 is 0.7208266357305989\n",
      "Loss at iteration 120 is 0.5805883856916022\n",
      "Loss at iteration 121 is 0.5419416263308089\n",
      "Loss at iteration 122 is 0.5606686808411524\n",
      "Loss at iteration 123 is 0.5431950652873359\n",
      "Loss at iteration 124 is 0.5539960962635236\n",
      "Loss at iteration 125 is 0.5944927809890662\n",
      "Loss at iteration 126 is 0.6494643890688768\n",
      "Loss at iteration 127 is 0.7035295925974987\n",
      "Loss at iteration 128 is 0.5742595380795981\n",
      "Loss at iteration 129 is 0.5417487834247018\n",
      "Loss at iteration 130 is 0.561941617342567\n",
      "Loss at iteration 131 is 0.6082287930338838\n",
      "Loss at iteration 132 is 0.6675939570312\n",
      "Loss at iteration 133 is 0.7180282337269469\n",
      "Loss at iteration 134 is 0.5900106163440633\n",
      "Loss at iteration 135 is 0.6525073289966202\n",
      "Loss at iteration 136 is 0.5590305009757679\n",
      "Loss at iteration 137 is 0.5530824040792053\n",
      "Loss at iteration 138 is 0.5880836556764373\n",
      "Loss at iteration 139 is 0.6390555110378136\n",
      "Loss at iteration 140 is 0.6939144377905643\n",
      "Loss at iteration 141 is 0.5807305895057646\n",
      "Loss at iteration 142 is 0.625568472277738\n",
      "Loss at iteration 143 is 0.6922065961095637\n",
      "Loss at iteration 144 is 0.7447430764600764\n",
      "Loss at iteration 145 is 0.7785838240070342\n",
      "Loss at iteration 146 is 0.8210116027052016\n",
      "Loss at iteration 147 is 0.8615859694998618\n",
      "Loss at iteration 148 is 0.8931604042256471\n",
      "Loss at iteration 149 is 0.9206344973906443\n",
      "Loss at iteration 150 is 0.6651764160829673\n",
      "Loss at iteration 151 is 0.5611734707666975\n",
      "Loss at iteration 152 is 0.6057987800729402\n",
      "Loss at iteration 153 is 0.6598904125837322\n",
      "Loss at iteration 154 is 0.5599655755438863\n",
      "Loss at iteration 155 is 0.5429668485662734\n",
      "Loss at iteration 156 is 0.5549249888698262\n",
      "Loss at iteration 157 is 0.5453075332205849\n",
      "Loss at iteration 158 is 0.5528266331287254\n",
      "Loss at iteration 159 is 0.5453428371219546\n",
      "Loss at iteration 160 is 0.5799863448329715\n",
      "Loss at iteration 161 is 0.6306547620263375\n",
      "Loss at iteration 162 is 0.5517373623697438\n",
      "Loss at iteration 163 is 0.5899473170032892\n",
      "Loss at iteration 164 is 0.5435848781398114\n",
      "Loss at iteration 165 is 0.5705607454353377\n",
      "Loss at iteration 166 is 0.5417298604967248\n",
      "Loss at iteration 167 is 0.563604818629916\n",
      "Loss at iteration 168 is 0.5437384600891723\n",
      "Loss at iteration 169 is 0.5678432300612135\n",
      "Loss at iteration 170 is 0.5419255273510408\n",
      "Loss at iteration 171 is 0.5638256212961922\n",
      "Loss at iteration 172 is 0.5430589420742712\n",
      "Loss at iteration 173 is 0.5558168663711253\n",
      "Loss at iteration 174 is 0.5465934147061458\n",
      "Loss at iteration 175 is 0.5826395669193664\n",
      "Loss at iteration 176 is 0.6384297480343405\n",
      "Loss at iteration 177 is 0.5621248508506352\n",
      "Loss at iteration 178 is 0.5721220365141637\n",
      "Loss at iteration 179 is 0.586770636100748\n",
      "Loss at iteration 180 is 0.5507549710652627\n",
      "Loss at iteration 181 is 0.5715953325588553\n",
      "Loss at iteration 182 is 0.5431404346285691\n",
      "Loss at iteration 183 is 0.5594365571374865\n",
      "Loss at iteration 184 is 0.5452441300901839\n",
      "Loss at iteration 185 is 0.5744816803024521\n",
      "Loss at iteration 186 is 0.6324113072083704\n",
      "Loss at iteration 187 is 0.6982523525381741\n",
      "Loss at iteration 188 is 0.7408449716616231\n",
      "Loss at iteration 189 is 0.801843696055322\n",
      "Loss at iteration 190 is 0.6065045100829817\n",
      "Loss at iteration 191 is 0.5487148990732004\n",
      "Loss at iteration 192 is 0.5631318239689886\n",
      "Loss at iteration 193 is 0.5902550295648985\n",
      "Loss at iteration 194 is 0.6433002748598519\n",
      "Loss at iteration 195 is 0.7008905851907131\n",
      "Loss at iteration 196 is 0.7592669904443309\n",
      "Loss at iteration 197 is 0.5936334283454578\n",
      "Loss at iteration 198 is 0.6453966325749595\n",
      "Loss at iteration 199 is 0.5548765544548939\n"
     ]
    }
   ],
   "source": [
    "arr = create_data()\n",
    "\n",
    "ans = gradient_descent(arr, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3cb8ea",
   "metadata": {},
   "source": [
    "We can see that the value of Argmin is for (eta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e28f4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5006413120437608, -0.16246933405480846)\n"
     ]
    }
   ],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fc89b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss: 1.169175\n",
      "Loss at iteration 0 is 1.131975\n",
      "Loss at iteration 1 is 0.9365469999999998\n",
      "Loss at iteration 2 is 0.9472509644444456\n",
      "Loss at iteration 3 is 0.8724525462444436\n",
      "Loss at iteration 4 is 0.8700813645388793\n",
      "Loss at iteration 5 is 0.8692053616191092\n",
      "Loss at iteration 6 is 0.8694375872994478\n",
      "Loss at iteration 7 is 0.8350995285478244\n",
      "Loss at iteration 8 is 0.8043455117474342\n",
      "Loss at iteration 9 is 0.7791172493391904\n",
      "Loss at iteration 10 is 0.7612569043058193\n",
      "Loss at iteration 11 is 0.7429247787562729\n",
      "Loss at iteration 12 is 0.7445067269121882\n",
      "Loss at iteration 13 is 0.7464036844711482\n",
      "Loss at iteration 14 is 0.7337010376370814\n",
      "Loss at iteration 15 is 0.7205631312219974\n",
      "Loss at iteration 16 is 0.7106156270357278\n",
      "Loss at iteration 17 is 0.7123122409952444\n",
      "Loss at iteration 18 is 0.7023570651923561\n",
      "Loss at iteration 19 is 0.6933379200998827\n",
      "Loss at iteration 20 is 0.6851548833932546\n",
      "Loss at iteration 21 is 0.6864798700545682\n",
      "Loss at iteration 22 is 0.6790825439360006\n",
      "Loss at iteration 23 is 0.6806505346881556\n",
      "Loss at iteration 24 is 0.673880162045884\n",
      "Loss at iteration 25 is 0.6758955212871947\n",
      "Loss at iteration 26 is 0.6772858638751731\n",
      "Loss at iteration 27 is 0.6714815956142338\n",
      "Loss at iteration 28 is 0.6660110562601502\n",
      "Loss at iteration 29 is 0.6607849228370445\n",
      "Loss at iteration 30 is 0.6616591839290179\n",
      "Loss at iteration 31 is 0.6634804020860104\n",
      "Loss at iteration 32 is 0.6646711274914768\n",
      "Loss at iteration 33 is 0.6604293140459246\n",
      "Loss at iteration 34 is 0.6567676294401993\n",
      "Loss at iteration 35 is 0.6528148266537912\n",
      "Loss at iteration 36 is 0.6539997336405102\n",
      "Loss at iteration 37 is 0.6551021770956703\n",
      "Loss at iteration 38 is 0.6565162759662383\n",
      "Loss at iteration 39 is 0.6528640983388765\n",
      "Loss at iteration 40 is 0.6536839312703475\n",
      "Loss at iteration 41 is 0.6548431791197008\n",
      "Loss at iteration 42 is 0.651927510467477\n",
      "Loss at iteration 43 is 0.6489204371060505\n",
      "Loss at iteration 44 is 0.6502515483929626\n",
      "Loss at iteration 45 is 0.6473111869914903\n",
      "Loss at iteration 46 is 0.6449275276958945\n",
      "Loss at iteration 47 is 0.6425625278479995\n",
      "Loss at iteration 48 is 0.6432988924010007\n",
      "Loss at iteration 49 is 0.6406384827979689\n",
      "Loss at iteration 50 is 0.6381653428198207\n",
      "Loss at iteration 51 is 0.6389002958111555\n",
      "Loss at iteration 52 is 0.6368117997475944\n",
      "Loss at iteration 53 is 0.637380006201015\n",
      "Loss at iteration 54 is 0.6382080700572145\n",
      "Loss at iteration 55 is 0.6363185015754524\n",
      "Loss at iteration 56 is 0.6369500658017574\n",
      "Loss at iteration 57 is 0.6351490119493919\n",
      "Loss at iteration 58 is 0.6329305770919594\n",
      "Loss at iteration 59 is 0.6310212444035629\n",
      "Loss at iteration 60 is 0.6292702901610737\n",
      "Loss at iteration 61 is 0.6276252151537568\n",
      "Loss at iteration 62 is 0.6283382557802402\n",
      "Loss at iteration 63 is 0.6290968292914192\n",
      "Loss at iteration 64 is 0.6298153799202209\n",
      "Loss at iteration 65 is 0.628296580391271\n",
      "Loss at iteration 66 is 0.6291266333425434\n",
      "Loss at iteration 67 is 0.6295284953779526\n",
      "Loss at iteration 68 is 0.6278418513851161\n",
      "Loss at iteration 69 is 0.6264779675998535\n",
      "Loss at iteration 70 is 0.6247350588408741\n",
      "Loss at iteration 71 is 0.6252157065356342\n",
      "Loss at iteration 72 is 0.6237892966291395\n",
      "Loss at iteration 73 is 0.6244307995684296\n",
      "Loss at iteration 74 is 0.6250229461279602\n",
      "Loss at iteration 75 is 0.6257408526844135\n",
      "Loss at iteration 76 is 0.6261767737724213\n",
      "Loss at iteration 77 is 0.6247049270829073\n",
      "Loss at iteration 78 is 0.623441416854485\n",
      "Loss at iteration 79 is 0.622193197317954\n",
      "Loss at iteration 80 is 0.6207026642230546\n",
      "Loss at iteration 81 is 0.6193447600912344\n",
      "Loss at iteration 82 is 0.6198666952934239\n",
      "Loss at iteration 83 is 0.6203325619519301\n",
      "Loss at iteration 84 is 0.6190257021418261\n",
      "Loss at iteration 85 is 0.6193721255732902\n",
      "Loss at iteration 86 is 0.6180531118814369\n",
      "Loss at iteration 87 is 0.6170103922329688\n",
      "Loss at iteration 88 is 0.6158130092484339\n",
      "Loss at iteration 89 is 0.6148499838473578\n",
      "Loss at iteration 90 is 0.6152554877305314\n",
      "Loss at iteration 91 is 0.6142444079556575\n",
      "Loss at iteration 92 is 0.6130787857939218\n",
      "Loss at iteration 93 is 0.6119279394853244\n",
      "Loss at iteration 94 is 0.6122770729486257\n",
      "Loss at iteration 95 is 0.611200394034133\n",
      "Loss at iteration 96 is 0.61168700797074\n",
      "Loss at iteration 97 is 0.6107119614205594\n",
      "Loss at iteration 98 is 0.611012031370688\n",
      "Loss at iteration 99 is 0.6100445412621579\n",
      "Loss at iteration 100 is 0.6104818115622787\n",
      "Loss at iteration 101 is 0.6109948243929438\n",
      "Loss at iteration 102 is 0.6112796462120672\n",
      "Loss at iteration 103 is 0.6102443675143029\n",
      "Loss at iteration 104 is 0.6106256256961828\n",
      "Loss at iteration 105 is 0.6096237924548817\n",
      "Loss at iteration 106 is 0.6086981477572863\n",
      "Loss at iteration 107 is 0.6078516031572719\n",
      "Loss at iteration 108 is 0.6082935932306847\n",
      "Loss at iteration 109 is 0.607463607149447\n",
      "Loss at iteration 110 is 0.6065121313049006\n",
      "Loss at iteration 111 is 0.606915199799458\n",
      "Loss at iteration 112 is 0.6073673738076741\n",
      "Loss at iteration 113 is 0.6078409870946119\n",
      "Loss at iteration 114 is 0.6070629897070494\n",
      "Loss at iteration 115 is 0.6073269172810754\n",
      "Loss at iteration 116 is 0.6065681215055715\n",
      "Loss at iteration 117 is 0.6068089983378421\n",
      "Loss at iteration 118 is 0.6060570148654045\n",
      "Loss at iteration 119 is 0.6052713251753146\n",
      "Loss at iteration 120 is 0.6056450701676563\n",
      "Loss at iteration 121 is 0.6059551597555024\n",
      "Loss at iteration 122 is 0.6051979855146546\n",
      "Loss at iteration 123 is 0.6043975252859959\n",
      "Loss at iteration 124 is 0.6048176613276522\n",
      "Loss at iteration 125 is 0.6050868219925182\n",
      "Loss at iteration 126 is 0.6043897961848536\n",
      "Loss at iteration 127 is 0.6037036291154096\n",
      "Loss at iteration 128 is 0.6040204145159208\n",
      "Loss at iteration 129 is 0.6044279491676323\n",
      "Loss at iteration 130 is 0.6037615298600338\n",
      "Loss at iteration 131 is 0.6040896997342032\n",
      "Loss at iteration 132 is 0.6033966926554684\n",
      "Loss at iteration 133 is 0.6028200333339931\n",
      "Loss at iteration 134 is 0.6020639841359546\n",
      "Loss at iteration 135 is 0.6013750281158425\n",
      "Loss at iteration 136 is 0.6016721241807418\n",
      "Loss at iteration 137 is 0.6010703326323131\n",
      "Loss at iteration 138 is 0.6004266888643085\n",
      "Loss at iteration 139 is 0.6007663246965275\n",
      "Loss at iteration 140 is 0.600966028143246\n",
      "Loss at iteration 141 is 0.6004159191377727\n",
      "Loss at iteration 142 is 0.6006391590647747\n",
      "Loss at iteration 143 is 0.6008840910082353\n",
      "Loss at iteration 144 is 0.6011391098518485\n",
      "Loss at iteration 145 is 0.6013492586625044\n",
      "Loss at iteration 146 is 0.6015914043027419\n",
      "Loss at iteration 147 is 0.6019436472422246\n",
      "Loss at iteration 148 is 0.6021460818483422\n",
      "Loss at iteration 149 is 0.601503832899229\n",
      "Loss at iteration 150 is 0.6018174194231808\n",
      "Loss at iteration 151 is 0.6012567378717246\n",
      "Loss at iteration 152 is 0.6015192881552639\n",
      "Loss at iteration 153 is 0.6009216128050153\n",
      "Loss at iteration 154 is 0.6003299267164369\n",
      "Loss at iteration 155 is 0.6006150752587033\n",
      "Loss at iteration 156 is 0.600810576140249\n",
      "Loss at iteration 157 is 0.6002981129294753\n",
      "Loss at iteration 158 is 0.6005713900326817\n",
      "Loss at iteration 159 is 0.6008091298191164\n",
      "Loss at iteration 160 is 0.600199565804431\n",
      "Loss at iteration 161 is 0.6004072920962737\n",
      "Loss at iteration 162 is 0.5998930363900841\n",
      "Loss at iteration 163 is 0.6001767725269598\n",
      "Loss at iteration 164 is 0.5996249082323347\n",
      "Loss at iteration 165 is 0.5990611235951495\n",
      "Loss at iteration 166 is 0.599366439467065\n",
      "Loss at iteration 167 is 0.5995555418527428\n",
      "Loss at iteration 168 is 0.5997653984711024\n",
      "Loss at iteration 169 is 0.5993145215239324\n",
      "Loss at iteration 170 is 0.5995320965897917\n",
      "Loss at iteration 171 is 0.5997815269887156\n",
      "Loss at iteration 172 is 0.5992261911830908\n",
      "Loss at iteration 173 is 0.598779535033054\n",
      "Loss at iteration 174 is 0.5982930855047142\n",
      "Loss at iteration 175 is 0.5985122176231565\n",
      "Loss at iteration 176 is 0.5987233396752774\n",
      "Loss at iteration 177 is 0.5982779061510048\n",
      "Loss at iteration 178 is 0.5984524776791562\n",
      "Loss at iteration 179 is 0.5987376690199873\n",
      "Loss at iteration 180 is 0.5982281214615486\n",
      "Loss at iteration 181 is 0.5985140528927886\n",
      "Loss at iteration 182 is 0.5986990779420269\n",
      "Loss at iteration 183 is 0.5989082779037591\n",
      "Loss at iteration 184 is 0.5991030974871763\n",
      "Loss at iteration 185 is 0.5993229716315864\n",
      "Loss at iteration 186 is 0.5995487042515798\n",
      "Loss at iteration 187 is 0.5990441496895308\n",
      "Loss at iteration 188 is 0.5985529105237459\n",
      "Loss at iteration 189 is 0.5980511766166429\n",
      "Loss at iteration 190 is 0.5983179907988443\n",
      "Loss at iteration 191 is 0.5979121436446657\n",
      "Loss at iteration 192 is 0.5981760396183672\n",
      "Loss at iteration 193 is 0.597747100064349\n",
      "Loss at iteration 194 is 0.5979132508452549\n",
      "Loss at iteration 195 is 0.5981692737139963\n",
      "Loss at iteration 196 is 0.5977569362281326\n",
      "Loss at iteration 197 is 0.5973160841658186\n",
      "Loss at iteration 198 is 0.596854698076044\n",
      "Loss at iteration 199 is 0.5970993128485557\n",
      "(0.6194648273810295, 0.3088399786979041)\n"
     ]
    }
   ],
   "source": [
    "#for eta=0.1 / (1 + t)\n",
    "ans = gradient_descent(arr, 200)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ea82a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss: 1.169175\n",
      "Loss at iteration 0 is 1.158307\n",
      "Loss at iteration 1 is 0.8991393117898516\n",
      "Loss at iteration 2 is 0.8983191226351087\n",
      "Loss at iteration 3 is 0.9184169680871596\n",
      "Loss at iteration 4 is 0.9069036144506777\n",
      "Loss at iteration 5 is 0.9245680505723105\n",
      "Loss at iteration 6 is 0.8188499328299704\n",
      "Loss at iteration 7 is 0.7319804137625765\n",
      "Loss at iteration 8 is 0.7499171693721913\n",
      "Loss at iteration 9 is 0.6865491484537611\n",
      "Loss at iteration 10 is 0.6388700019955135\n",
      "Loss at iteration 11 is 0.6079337324638101\n",
      "Loss at iteration 12 is 0.5871115536978077\n",
      "Loss at iteration 13 is 0.5747414536168735\n",
      "Loss at iteration 14 is 0.5805973232035303\n",
      "Loss at iteration 15 is 0.5857409529633071\n",
      "Loss at iteration 16 is 0.5717219535603724\n",
      "Loss at iteration 17 is 0.5789324568844004\n",
      "Loss at iteration 18 is 0.5695200033998308\n",
      "Loss at iteration 19 is 0.5759168755847129\n",
      "Loss at iteration 20 is 0.5647675179367974\n",
      "Loss at iteration 21 is 0.569471925306194\n",
      "Loss at iteration 22 is 0.5623549270872964\n",
      "Loss at iteration 23 is 0.5689390468348975\n",
      "Loss at iteration 24 is 0.5606860999734328\n",
      "Loss at iteration 25 is 0.5565961011166123\n",
      "Loss at iteration 26 is 0.560994771085593\n",
      "Loss at iteration 27 is 0.566583919915836\n",
      "Loss at iteration 28 is 0.5617490912192666\n",
      "Loss at iteration 29 is 0.567608508498533\n",
      "Loss at iteration 30 is 0.5727988942547269\n",
      "Loss at iteration 31 is 0.5671133570280502\n",
      "Loss at iteration 32 is 0.5637948075317097\n",
      "Loss at iteration 33 is 0.5572086442148689\n",
      "Loss at iteration 34 is 0.5606295099717535\n",
      "Loss at iteration 35 is 0.5589183928892987\n",
      "Loss at iteration 36 is 0.560061811557034\n",
      "Loss at iteration 37 is 0.5612921858015073\n",
      "Loss at iteration 38 is 0.5565128880702372\n",
      "Loss at iteration 39 is 0.5573768277494607\n",
      "Loss at iteration 40 is 0.5603462727759548\n",
      "Loss at iteration 41 is 0.5546991392170625\n",
      "Loss at iteration 42 is 0.551316059052139\n",
      "Loss at iteration 43 is 0.5529278945043717\n",
      "Loss at iteration 44 is 0.5489819047411679\n",
      "Loss at iteration 45 is 0.5501979203291864\n",
      "Loss at iteration 46 is 0.5530260461367892\n",
      "Loss at iteration 47 is 0.5507535893615565\n",
      "Loss at iteration 48 is 0.5469699240521271\n",
      "Loss at iteration 49 is 0.5490285001727748\n",
      "Loss at iteration 50 is 0.5473765511215671\n",
      "Loss at iteration 51 is 0.5466658702460284\n",
      "Loss at iteration 52 is 0.5465770879611064\n",
      "Loss at iteration 53 is 0.546811594044111\n",
      "Loss at iteration 54 is 0.5460889459077969\n",
      "Loss at iteration 55 is 0.5464168349036447\n",
      "Loss at iteration 56 is 0.5464629568351285\n",
      "Loss at iteration 57 is 0.5453627791357322\n",
      "Loss at iteration 58 is 0.5443283328889419\n",
      "Loss at iteration 59 is 0.5448572497748713\n",
      "Loss at iteration 60 is 0.5458761260002727\n",
      "Loss at iteration 61 is 0.5444560914727513\n",
      "Loss at iteration 62 is 0.5441582488802481\n",
      "Loss at iteration 63 is 0.5451465988514301\n",
      "Loss at iteration 64 is 0.5441796109276493\n",
      "Loss at iteration 65 is 0.5451631642996739\n",
      "Loss at iteration 66 is 0.546218198477042\n",
      "Loss at iteration 67 is 0.5451065587288423\n",
      "Loss at iteration 68 is 0.5448859425890212\n",
      "Loss at iteration 69 is 0.5458907844815233\n",
      "Loss at iteration 70 is 0.5479147552684305\n",
      "Loss at iteration 71 is 0.550081292602314\n",
      "Loss at iteration 72 is 0.5525586054208772\n",
      "Loss at iteration 73 is 0.5558636940982487\n",
      "Loss at iteration 74 is 0.5522835072734267\n",
      "Loss at iteration 75 is 0.5548369938233711\n",
      "Loss at iteration 76 is 0.5579081597158029\n",
      "Loss at iteration 77 is 0.5544040206969497\n",
      "Loss at iteration 78 is 0.5572095114564648\n",
      "Loss at iteration 79 is 0.5598461095429187\n",
      "Loss at iteration 80 is 0.5550528127908304\n",
      "Loss at iteration 81 is 0.557471742052222\n",
      "Loss at iteration 82 is 0.560294517607412\n",
      "Loss at iteration 83 is 0.5561924841628696\n",
      "Loss at iteration 84 is 0.5521348419552023\n",
      "Loss at iteration 85 is 0.5493887801966522\n",
      "Loss at iteration 86 is 0.5467642551530143\n",
      "Loss at iteration 87 is 0.5451180766413778\n",
      "Loss at iteration 88 is 0.5462286096792033\n",
      "Loss at iteration 89 is 0.5478215067418467\n",
      "Loss at iteration 90 is 0.549723275174891\n",
      "Loss at iteration 91 is 0.5469064748537567\n",
      "Loss at iteration 92 is 0.5450991018849927\n",
      "Loss at iteration 93 is 0.5470210072478835\n",
      "Loss at iteration 94 is 0.5487415808354601\n",
      "Loss at iteration 95 is 0.5463302238420836\n",
      "Loss at iteration 96 is 0.5442664992747501\n",
      "Loss at iteration 97 is 0.5458124699322249\n",
      "Loss at iteration 98 is 0.5440686787686303\n",
      "Loss at iteration 99 is 0.5429158060695995\n",
      "Loss at iteration 100 is 0.5421367172949455\n",
      "Loss at iteration 101 is 0.5428465429975062\n",
      "Loss at iteration 102 is 0.5439693918520792\n",
      "Loss at iteration 103 is 0.5451557686528182\n",
      "Loss at iteration 104 is 0.5435000142115384\n",
      "Loss at iteration 105 is 0.5426684963946479\n",
      "Loss at iteration 106 is 0.5436472615196054\n",
      "Loss at iteration 107 is 0.5448185523909856\n",
      "Loss at iteration 108 is 0.5437320234407599\n",
      "Loss at iteration 109 is 0.5426071912644634\n",
      "Loss at iteration 110 is 0.5419303156357013\n",
      "Loss at iteration 111 is 0.5424722731068945\n",
      "Loss at iteration 112 is 0.5432669734290276\n",
      "Loss at iteration 113 is 0.5441754719273014\n",
      "Loss at iteration 114 is 0.5454685421701307\n",
      "Loss at iteration 115 is 0.5470334897997731\n",
      "Loss at iteration 116 is 0.5451583462598782\n",
      "Loss at iteration 117 is 0.5466906665274093\n",
      "Loss at iteration 118 is 0.5484816196054858\n",
      "Loss at iteration 119 is 0.5504708927329595\n",
      "Loss at iteration 120 is 0.5479616791157726\n",
      "Loss at iteration 121 is 0.5459712307684215\n",
      "Loss at iteration 122 is 0.5443634188444892\n",
      "Loss at iteration 123 is 0.5432563721361305\n",
      "Loss at iteration 124 is 0.5423186931325076\n",
      "Loss at iteration 125 is 0.5430418052428783\n",
      "Loss at iteration 126 is 0.5440919952320865\n",
      "Loss at iteration 127 is 0.5430259075332567\n",
      "Loss at iteration 128 is 0.5439478025496192\n",
      "Loss at iteration 129 is 0.5451090342208492\n",
      "Loss at iteration 130 is 0.5436888928372036\n",
      "Loss at iteration 131 is 0.5427053453322389\n",
      "Loss at iteration 132 is 0.54210272189329\n",
      "Loss at iteration 133 is 0.5418034988691368\n",
      "Loss at iteration 134 is 0.5420279651451163\n",
      "Loss at iteration 135 is 0.5425848743414715\n",
      "Loss at iteration 136 is 0.5434516530609689\n",
      "Loss at iteration 137 is 0.5425142901314308\n",
      "Loss at iteration 138 is 0.5419443954855749\n",
      "Loss at iteration 139 is 0.5416856131771961\n",
      "Loss at iteration 140 is 0.5419277596971395\n",
      "Loss at iteration 141 is 0.5416923415033749\n",
      "Loss at iteration 142 is 0.5419642028388139\n",
      "Loss at iteration 143 is 0.5424942880318515\n",
      "Loss at iteration 144 is 0.5420905267172131\n",
      "Loss at iteration 145 is 0.5427571180516335\n",
      "Loss at iteration 146 is 0.5420724508965555\n",
      "Loss at iteration 147 is 0.5417731690456961\n",
      "Loss at iteration 148 is 0.5419761783983487\n",
      "Loss at iteration 149 is 0.5417652050624288\n",
      "Loss at iteration 150 is 0.5419671451104685\n",
      "Loss at iteration 151 is 0.5424004375898348\n",
      "Loss at iteration 152 is 0.5421645431104275\n",
      "Loss at iteration 153 is 0.5425882550167249\n",
      "Loss at iteration 154 is 0.5425165058789728\n",
      "Loss at iteration 155 is 0.5425952277464456\n",
      "Loss at iteration 156 is 0.5425298877514058\n",
      "Loss at iteration 157 is 0.5432273474732909\n",
      "Loss at iteration 158 is 0.5430016683865905\n",
      "Loss at iteration 159 is 0.5428667482087028\n",
      "Loss at iteration 160 is 0.5431442347457888\n",
      "Loss at iteration 161 is 0.5433548041594719\n",
      "Loss at iteration 162 is 0.5442625896945903\n",
      "Loss at iteration 163 is 0.5437693040067064\n",
      "Loss at iteration 164 is 0.5443407567983388\n",
      "Loss at iteration 165 is 0.5440175435618184\n",
      "Loss at iteration 166 is 0.5442582344908994\n",
      "Loss at iteration 167 is 0.5438369907311543\n",
      "Loss at iteration 168 is 0.5439499647096782\n",
      "Loss at iteration 169 is 0.544857413993157\n",
      "Loss at iteration 170 is 0.5442605923663156\n",
      "Loss at iteration 171 is 0.543813495739862\n",
      "Loss at iteration 172 is 0.5433566768267276\n",
      "Loss at iteration 173 is 0.5432436685272314\n",
      "Loss at iteration 174 is 0.5435367914861264\n",
      "Loss at iteration 175 is 0.543543949001715\n",
      "Loss at iteration 176 is 0.5435337835587373\n",
      "Loss at iteration 177 is 0.5442329969062001\n",
      "Loss at iteration 178 is 0.5435587663329146\n",
      "Loss at iteration 179 is 0.5430648007818134\n",
      "Loss at iteration 180 is 0.5434031268220662\n",
      "Loss at iteration 181 is 0.5435930871857579\n",
      "Loss at iteration 182 is 0.5434713401536587\n",
      "Loss at iteration 183 is 0.5436009639335854\n",
      "Loss at iteration 184 is 0.5434681419125923\n",
      "Loss at iteration 185 is 0.5430527455664941\n",
      "Loss at iteration 186 is 0.5430800556317514\n",
      "Loss at iteration 187 is 0.5430112844916544\n",
      "Loss at iteration 188 is 0.5435232135414875\n",
      "Loss at iteration 189 is 0.5442622607323451\n",
      "Loss at iteration 190 is 0.5441508578814307\n",
      "Loss at iteration 191 is 0.5445743291242547\n",
      "Loss at iteration 192 is 0.5446245832659367\n",
      "Loss at iteration 193 is 0.5456504835162166\n",
      "Loss at iteration 194 is 0.5448083069805891\n",
      "Loss at iteration 195 is 0.5445434000222705\n",
      "Loss at iteration 196 is 0.544709596220429\n",
      "Loss at iteration 197 is 0.5449403155511795\n",
      "Loss at iteration 198 is 0.5439731320343334\n",
      "Loss at iteration 199 is 0.5448179983510032\n",
      "(0.4378182387646616, -0.055147464892148106)\n"
     ]
    }
   ],
   "source": [
    "# for eta=0.1 / pow((1 + t), 0.5) \n",
    "ans = gradient_descent(arr, 200)\n",
    "print(ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
